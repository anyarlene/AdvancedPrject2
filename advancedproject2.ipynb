{"cells":[{"metadata":{},"cell_type":"markdown","source":"# How do PV infeed change phasor angle information?"},{"metadata":{},"cell_type":"markdown","source":"#### Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing packages that may be needed:\n\nimport pandas as pd # data processing\nimport numpy as np # linear algebra\nimport math\nimport dask.dataframe as dd\nimport os\nfrom tqdm import tqdm\n\nimport statsmodels.api as sm\n#import statsmodels.api as sm\n\nimport matplotlib.pyplot as plt # this is used for the plot the graph\nimport plotly.express as px # this is used for the plot the graph\nimport seaborn as sns # used for plot interactive graph.\nfrom matplotlib import pylab\n\nfrom sklearn.metrics import r2_score\n\nimport sys \nfrom scipy.stats import randint\nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.model_selection import KFold # use for cross validation\nfrom sklearn.preprocessing import StandardScaler # for normalization\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline # pipeline making\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics # for the check the error and accuracy of the model\nfrom sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n\n## for Deep-learing:\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD \nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nimport itertools\nfrom keras.layers import LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Energy solar data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path1 = '../input/pv-data/solar-15m_2019-09-01_2020-09-30.csv'\nPV_df = pd.read_csv(path1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PV_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PV_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PV_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(PV_df.head())\ndisplay(PV_df.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing values in each column of dataframe\nmissing_val = (PV_df.isna().sum())\nprint(\"missing values in each column: \\n\",missing_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PV_df['time'] = pd.to_datetime(PV_df['time'], format='%Y-%m-%d %H:%M')\n\nPV_df['Date'] = [d.date() for d in PV_df['time']]\nPV_df['Time'] = [d.time() for d in PV_df['time']]\n\nPV_df = PV_df.set_index(['time'])\nPV_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start and end dates of the time series\nprint (\"solar energy dataset: start_date = {}, end_date = {} \\n\".format(PV_df.index.min(), PV_df.index.max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nPV_df.plot(grid =True, subplots=True, figsize = (20, 20))\nplt.xlabel('time', fontsize=16)\nplt.legend(loc=\"upper left\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PV_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PV_df = PV_df.reset_index(['time'])\n#aggregate_pv_df = PV_df.copy()[['Date', 'Time']]\naggregate_pv_df = PV_df.copy()[['time']]\naggregate_pv_df['Hochrechnung_Total'] =  PV_df[['Solarenergie_Hochrechnung_50Hertz','Solarenergie_Hochrechnung_Amprion', 'Solarenergie_Hochrechnung_TenneT_TSO', 'Solarenergie_Hochrechnung_TransnetBW']].sum(axis=1)\naggregate_pv_df['Prognose_Total'] =  PV_df[['Solarenergie_Prognose_50Hertz','Solarenergie_Prognose_Amprion', 'Solarenergie_Prognose_TenneT_TSO', 'Solarenergie_Prognose_TransnetBW']].sum(axis=1)\n\naggregate_pv_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aggregate_pv_df = aggregate_pv_df.set_index(['time'])\naggregate_pv_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start and end dates of the time series\nprint (\"PV dataset: start_date = {}, end_date = {} \\n\".format(aggregate_pv_df.index.min(), aggregate_pv_df.index.max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aggregate_pv_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aggregate_pv_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\naggregate_pv_df['Hochrechnung_Total'].plot(grid = True, figsize = (16, 9))\naggregate_pv_df['Prognose_Total'].plot(grid = True, figsize = (16, 9))\nplt.xlabel('time', fontsize=16)\nplt.legend(loc=\"upper left\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Heatmap visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"pv_heatmap_df = aggregate_pv_df.copy()\npv_heatmap_df = pv_heatmap_df.reset_index()\n\ntime_series_2020 = (pv_heatmap_df['time'] >= ('2019-09-01')) & (pv_heatmap_df['time'] <= ('2020-09-30'))\nfrom_jan_2020 = pv_heatmap_df.loc[time_series_2020]\nfrom_jan_2020 = from_jan_2020.set_index(['time'])\n\n#Create variables for Day and hour\nfrom_jan_2020['day'] = [i.dayofyear for i in from_jan_2020.index]\nfrom_jan_2020['hour'] = [i.hour for i in from_jan_2020.index]\n\n# group by month and year, get the average\nfrom_jan_2020 = from_jan_2020.groupby(['day', 'hour']).mean()\n\nfrom_jan_2020= from_jan_2020['Hochrechnung_Total'].unstack(level=0)\n\nfig, ax = plt.subplots(figsize=(25, 10))\n#cmap = \"Reds\"\n#cmap = \"RdPu\"\n#cmap = \"YlOrRd\"\n#cmap = \"autumn\"\n#cmap = \"hot\"\n#cmap=\"OrRd\"\n#cmap = \"Oranges\"\nsns.heatmap(from_jan_2020, vmin=0)\nplt.xlabel(\"Day\", fontsize=14)\nplt.ylabel(\"Hour of the Day\", fontsize=14)\n\n# This sets the yticks \"upright\" with 0, as opposed to sideways with 90.\nplt.yticks(rotation=0)\n#plt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pv_heatmap_df2 = aggregate_pv_df.copy()\npv_heatmap_df2 = pv_heatmap_df2.reset_index()\n\ntime_series_2020 = (pv_heatmap_df2['time'] >= ('2020-01-01')) & (pv_heatmap_df2['time'] <= ('2020-09-30'))\nfrom_jan_2020 = pv_heatmap_df2.loc[time_series_2020]\nfrom_jan_2020 = from_jan_2020.set_index(['time'])\n\n#Create variables for Day and hour\nfrom_jan_2020['month'] = [i.month for i in from_jan_2020.index]\nfrom_jan_2020['hour'] = [i.hour for i in from_jan_2020.index]\n\n# group by month and year, get the average\nfrom_jan_2020 = from_jan_2020.groupby(['month', 'hour']).mean()\nfrom_jan_2020 = from_jan_2020['Hochrechnung_Total'].unstack(level=0)\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\n#cmap = \"Reds\"\n#cmap = \"RdPu\"\n#cmap=\"OrRd\"\n#cmap = \"Oranges\"\n\n#import cmocean\n#cmap = cmocean.cm.oxy\n#plt.contourf(from_jan_2020, 20, cmap=cmap)\n\nsns.heatmap(from_jan_2020, vmin=0)\nplt.title('Year 2020', fontsize=14, fontweight='bold')\nplt.xlabel(\"Month_Number\", fontsize=14)\nplt.ylabel(\"Hour of the Day\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Frequency Time Series Decomposition\n\nThe frequency of decomposition must be an interval, which 'may' repeat. So we have data with 15min frequency and we are looking for a weekly repetition behavior.\nTo look for a weekly repetition behavior, let's use:\n$decompfreq = \\cfrac{24h \\cdot 60min}{15min} \\cdot 7days$"},{"metadata":{"trusted":true},"cell_type":"code","source":"decompfreq = int(((24*60)/15)*7)\nres = sm.tsa.seasonal_decompose(aggregate_pv_df.Hochrechnung_Total.interpolate(),\n                               period=decompfreq,model='additive')\npylab.rcParams['figure.figsize'] = (14, 9)\nresplot = res.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Phasor angle data"},{"metadata":{},"cell_type":"markdown","source":"### Data Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"path2 = '../input/phasor-data/phase-angle-1s_2019-09-01_2020-09-30.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Peep at the training file header\ndf_tmp = pd.read_csv(path2, nrows=901, parse_dates =['time'], keep_date_col = True)\ndf_tmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val = (df_tmp.isna().sum())\nprint(missing_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's take locations Herzogenrath and Schondorf for demonstration purpose\ndf_tmp = df_tmp.set_index(['time'])  \n\ndf_tmp['phase_diff'] = df_tmp['Herzogenrath'] - df_tmp['Schondorf']\ndf_tmp['angle'] = np.sin(np.deg2rad(df_tmp['phase_diff']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing 15 min interval of the time series"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(x=df_tmp.index, y=df_tmp['Herzogenrath'], title='site1 \\n in degrees')\nfig.update_xaxes()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(x=df_tmp.index, y=df_tmp['phase_diff'], title='site1 - site2 \\n  in degrees')\nfig.update_xaxes()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(x=df_tmp.index, y=df_tmp['angle'], title='sin (site1 - site2 \\n)')\nfig.update_xaxes()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Locations\nsite1 = \"Winterthur\"\nsite2 = \"Büdingen\"\nsite3 = \"Schondorf\"\nsite4 = \"Herzogenrath\"\nsite5 = \"Bremen\"\nsite6=  \"Dresden\"\nsite7 = \"Lleida\"\nsite8 = \"Sibiu\"\nsite9 = \"Belfort Cedex\"\nsite10 = \"Wien (SBA)\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chunksize = 10000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_list = [] # list to hold the batch dataframe\n\nfor df_chunk in tqdm(pd.read_csv(path2, usecols=(\"time\",site1, site2,site3,site4,site5,site6,site7,site8,site9,site10),parse_dates =['time'], keep_date_col = True, chunksize=chunksize)):\n     \n    df_chunk = df_chunk.set_index(['time'])\n    #df_chunk['time'] = pd.to_datetime(df_chunk['time'], utc=True, format='%Y-%m-%d %H:%M:%S')\n    \n    # Alternatively, append the chunk to list and merge all\n    df_list.append(df_chunk)\n    \n\n# Merge all dataframes into one dataframe\ndata = pd.concat(df_list)\n\n# Delete the dataframe list to release memory\ndel df_list\n\n# See what we have loaded\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start and end dates of the time series\nprint (\"phase angle dataset: start_date = {}, end_date = {} \\n\".format(data.index.min(), data.index.max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val = (data.isna().sum())\nprint(missing_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_list = [] # list to hold the batch dataframe\n\nfor df_chunk in tqdm(pd.read_csv(path2, usecols=(\"time\",site1, site2,site3,site4,site5,site6,site7,site8,site9,site10),parse_dates =['time'], keep_date_col = True, chunksize=chunksize)):\n    #We will resample the data to 15 min to be able to plot; dataset is too large and require additional memory to process\n\n    df_chunk = df_chunk.set_index(['time'])\n    df_chunk = df_chunk.resample('15 min').mean()\n    \n    \n    # Alternatively, append the chunk to list and merge all\n    df_list.append(df_chunk)\n    \n\n# Merge all dataframes into one dataframe\ndata_df = pd.concat(df_list)\n\n# Delete the dataframe list to release memory\ndel df_list\n\n# See what we have loaded\ndata_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val = (data_df.isna().sum())\nprint(missing_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndata_df.plot(grid=True, subplots=True, figsize=(18,18))\nplt.xlabel('time', fontsize=18)\nplt.legend(loc='upper left')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's choose regions in Germany located in opposite geographical directions that to say Bremen(North), Herzogenrath(North), Büdingen(South-East) and Schondorf(South) for the project.\n#### Notice that Bremen appears to miss data from misses from September 2019 to January 2020."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Locations chosen for the project\nsite2 = \"Büdingen\"\nsite3 = \"Schondorf\"\nsite4 = \"Herzogenrath\"\nsite5 = \"Bremen\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_list = [] # list to hold the batch dataframe\n\nfor df_chunk in tqdm(pd.read_csv(path2, usecols=(\"time\",site2,site3,site4,site5),parse_dates =['time'], keep_date_col = True, chunksize=chunksize)):\n    #We will resample the data to 15 min to be able to plot; dataset is too large and require additional memory to process\n\n    df_chunk = df_chunk.set_index(['time'])\n    df_chunk = df_chunk.resample('15 min').mean()\n    \n    # Append the chunk to list and merge all\n    df_list.append(df_chunk)\n    \n\n# Merge all dataframes into one dataframe\ndf_data = pd.concat(df_list)\n\n# Delete the dataframe list to release memory\ndel df_list\n\n# See what we have loaded\ndf_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start and end dates of the time series\nprint (\"phase angle dataset: start_date = {}, end_date = {} \\n\".format(df_data.index.min(), df_data.index.max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val = (df_data.isna().sum())\nprint(missing_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_data.plot(grid=True, subplots=True, figsize=(18,18))\nplt.xlabel('time', fontsize=18)\nplt.legend(loc='upper left')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bremen data starts around the beginning of January"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_list = [] # list to hold the batch dataframe\n\nfor df_chunk in tqdm(pd.read_csv(path2, usecols=(\"time\",site2,site3,site4,site5),parse_dates =['time'], keep_date_col = True, chunksize=chunksize)):\n    \n    df_chunk = df_chunk.set_index(['time'])  \n    # Can process each chunk of dataframe here\n    # clean_data(), pre_process_data()\n    \n    #df_chunk = df_chunk.assign(Büdingen2=df_chunk.Büdingen.fillna(df_chunk.Büdingen.mean()))\n    df_chunk = df_chunk.assign(Herzogenrath2=df_chunk.Herzogenrath.fillna(df_chunk.Herzogenrath.mean()))\n    df_chunk = df_chunk.assign(Schondorf2=df_chunk.Schondorf.fillna(df_chunk.Schondorf.mean()))\n   \n    df_chunk['phase_diff1'] = (df_chunk['Bremen'] - df_chunk['Schondorf2'])\n    df_chunk['angle1'] = np.sin(np.deg2rad(df_chunk['phase_diff1']))\n    \n    \n    df_chunk['phase_diff2'] = (df_chunk['Herzogenrath2'] - df_chunk['Schondorf2'])\n    df_chunk['angle2'] = np.sin(np.deg2rad(df_chunk['phase_diff2']))\n    \n    df_chunk['phase_diff3'] = (df_chunk['Bremen'] - df_chunk['Büdingen'])\n    df_chunk['angle3'] = np.sin(np.deg2rad(df_chunk['phase_diff3']))\n    \n    #df_chunk['time'] = pd.to_datetime(df_chunk['time'], format='%Y-%m-%d %H:%M:%S')\n    df_chunk = df_chunk.resample('15 min').mean()\n    #df2_chunk = df2_chunk.set_index(['time'])\n\n    \n    # Append the chunk to list and merge all\n    df_list.append(df_chunk)\n\n# Merge all dataframes into one dataframe\nphasor_data = pd.concat(df_list)\n\n# Delete the dataframe list to release memory\ndel df_list\n\n# See what I have loaded\nphasor_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phasor_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start and end dates of the time series\nprint (\"phase angle dataset: start_date = {}, end_date = {} \\n\".format(phasor_data.index.min(), phasor_data.index.max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val = (phasor_data.isna().sum())\nprint(missing_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phasor_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phasor_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nphasor_data.plot(grid=True, subplots=True, figsize=(18,18))\nplt.xlabel('time', fontsize=18)\nplt.legend(loc='upper left')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's use the time series from Jan 2020, since Bremen misses data from September 2019 until Jan 7th, 2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"angle_data = phasor_data[['angle1', 'angle2', 'angle3']].copy()\n#angle_data.head()\n\nphasor = angle_data.copy()\nphasor = phasor.reset_index()\ntime_df = (phasor['time'] >= ('2020-01-07 00:00:00')) & (phasor['time'] <= ('2020-09-30 23:45:00'))\nphasor_df = phasor.loc[time_df]\nphasor_df = phasor_df.set_index(['time'])\n\nprint(\"angle1 : Bremen_Schondorf\")\nprint(\"angle2: Herzogenrath_Schondorf\")\nprint(\"angle3: Bremen_Büdingen\")\n\nphasor_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phasor_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization of power flow transfer between 2 locations"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = px.line(x=phasor_df.index, y=phasor_df['angle2'], title='Power flow transfer between Herzogenrath & Schondorf')\nfig1.update_xaxes(rangeslider_visible=True)\nfig1.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2 = px.line(x=phasor_df.index, y=phasor_df['angle1'], title='Power flow transfer between Bremen & Schondorf')\nfig2.update_xaxes(rangeslider_visible=True)\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig3 = px.line(x=phasor_df.index, y=phasor_df['angle3'], title='Power flow transfer between Bremen & Büdingen')\nfig3.update_xaxes(rangeslider_visible=True)\nfig3.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start and end dates of the time series\nprint (\"phase angle dataset: start_date = {}, end_date = {} \\n\".format(phasor_df.index.min(), phasor_df.index.max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Heatmap visualization between 2 locations"},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap_df1 = phasor_df.copy()\n\n#Create variables for Day and hour\nheatmap_df1['Day'] = [i.dayofyear for i in heatmap_df1.index]\nheatmap_df1['hour'] = [i.hour for i in heatmap_df1.index]\n\n#Group by day and hour and aggregate\nheatmap_df1 = heatmap_df1.groupby(['Day','hour']).mean()\n\n#Use unstack function to prepare the data to be plotted\nheatmap_df1= heatmap_df1['angle1'].unstack(level=0)\n\nfig1, ax = plt.subplots(figsize=(25, 6))\n\nfig1.canvas.draw()\ncmap = 'RdYlBu'\n\n\nsns.heatmap(heatmap_df1, cmap=cmap)\n\n#plt.title('Herzogenrath_Schondorf \\n (Jan 2020 - Sept 2020)', fontweight='bold', fontsize = 14)\nplt.title('Bremen_Schondorf', fontweight='bold', fontsize = 14)\nplt.xlabel(\"Day_Number\", fontsize=12)\nplt.ylabel(\"Hour of the day\", fontsize=12)\n\nplt.yticks(rotation=0)\n#plt.xticks('Day -' + from_jan_2020.Day)\n\nlabels = [item.get_text() for item in ax.get_xticklabels()]\n#labels[2] = 'Day -'\n\nax.set_xticklabels(labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap_df2 = phasor_df.copy()\n\n#Create variables for Day and hour\nheatmap_df2['day'] = [i.dayofyear for i in heatmap_df2.index]\nheatmap_df2['hour'] = [i.hour for i in heatmap_df2.index]\n\n#Group by day and hour and aggregate\nheatmap_df2 = heatmap_df2.groupby(['day','hour']).mean()\n\n#Use unstack function to prepare the data to be plotted\nheatmap_df2= heatmap_df2['angle2'].unstack(level=0)\n\n\nfig2, ax = plt.subplots(figsize=(25, 6))\n\nfig2.canvas.draw()\ncmap = 'RdYlBu'\n\n\nsns.heatmap(heatmap_df2, cmap=cmap)\n\n#plt.title('Herzogenrath_Schondorf \\n (Jan 2020 - Sept 2020)', fontweight='bold', fontsize = 14)\nplt.title('Herzogenrath_Schondorf', fontweight='bold', fontsize = 14)\nplt.xlabel(\"Day_Number\", fontsize=12)\nplt.ylabel(\"Hour of the day\", fontsize=12)\n\nplt.yticks(rotation=0)\n#plt.xticks('Day -' + from_jan_2020.Day)\n\nlabels = [item.get_text() for item in ax.get_xticklabels()]\n#labels[2] = 'Day -'\n\nax.set_xticklabels(labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap_df3 = phasor_df.copy()\n\n#Create variables for Day and hour\nheatmap_df3['Day'] = [i.dayofyear for i in heatmap_df3.index]\nheatmap_df3['hour'] = [i.hour for i in heatmap_df3.index]\n\n#Group by day and hour and aggregate\nheatmap_df3 = heatmap_df3.groupby(['Day','hour']).mean()\n\n\n#Use unstack function to prepare the data to be plotted\nheatmap_df3= heatmap_df3['angle3'].unstack(level=0)\n\n\nfig3, ax = plt.subplots(figsize=(25, 6))\n\nfig3.canvas.draw()\n#cmap = 'YlGnBu'\n#cmap = 'YlGnBu'\n#cmap = 'YlOrBr'\n#cmap = 'BuGn_r'\ncmap = 'RdYlBu'\n\n\nsns.heatmap(heatmap_df3, cmap=cmap)\n\n#plt.title('Herzogenrath_Schondorf \\n (Jan 2020 - Sept 2020)', fontweight='bold', fontsize = 14)\nplt.title('Bremen_Büdingen', fontweight='bold', fontsize = 14)\nplt.xlabel(\"Day_Number\", fontsize=12)\nplt.ylabel(\"Hour of the day\", fontsize=12)\n\nplt.yticks(rotation=0)\n#plt.xticks('Day -' + from_jan_2020.Day)\n\nlabels = [item.get_text() for item in ax.get_xticklabels()]\n#labels[2] = 'Day -'\n\nax.set_xticklabels(labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Frequency Time Series Decomposition"},{"metadata":{"trusted":true},"cell_type":"code","source":"decompfreq = int(((24*60)/15)*7)\nres = sm.tsa.seasonal_decompose(phasor_df.angle1.interpolate(),\n                               period=decompfreq,model='additive')\npylab.rcParams['figure.figsize'] = (14, 9)\nresplot = res.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation between solar energy and phasor angle"},{"metadata":{},"cell_type":"markdown","source":"#### Let's choose time series from Jan 2020 for solar energy as we did for phasor angle data"},{"metadata":{"trusted":true},"cell_type":"code","source":"aggregate_pv_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"energy = aggregate_pv_df.copy()\nenergy = energy.reset_index()\ntime_series = (energy['time'] >= ('2020-01-07 00:00:00')) & (energy['time'] <= ('2020-09-30 23:45:00'))\npv_df = energy.loc[time_series]\npv_df = pv_df.set_index(['time'])\npv_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pv_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start and end dates of the time series\nprint (\"PV dataset: start_date = {}, end_date = {} \\n\".format(pv_df.index.min(), pv_df.index.max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = phasor_df.copy()\ndata2 = pv_df.copy()\ndata1 = data1.reset_index()\ndata2 = data2.reset_index()\n\nall_data = data1.merge(data2, left_on=['time'], right_on=['time'], how='right')\n\n\n#data = data.drop(['Winterthur', 'Dresden','Lleida','Sibiu','Belfort Cedex','Wien (SBA)'], axis = 1)\ndf = all_data.copy()\ndf.rename(columns = {'angle1':'phasor_diff1','angle2':'phasor_diff2','angle3':'phasor_diff3',\n                   'Hochrechnung_Total':'PV_hoch', 'Prognose_Total':'PV_prog'}, inplace = True)\n#df.rename(columns = {'angle1':'phasor_diff1','angle2':'phasor_diff2',\n                   #'Hochrechnung_Total':'Solar_energy',\n                   #'Prognose_Total':'Prognose'}, inplace = True)\n#df.head()\nprint(\"phasor_diff1 : Bremen_Schondorf\")\nprint(\"phasor_diff2: Herzogenrath_Schondorf\")\nprint(\"phasor_diff3: Bremen_Büdingen\")\n\ncorr = df.corr().loc[['phasor_diff1', 'phasor_diff2', 'phasor_diff3','PV_hoch', 'PV_prog'], ['phasor_diff1', 'phasor_diff2','phasor_diff3', 'PV_hoch', 'PV_prog']]\ndisplay(corr)\n\n#corr = df.corr().loc[['phasor_angle1', 'energy_hoch'], ['phasor_angle1', 'energy_hoch']]\n#display(corr)\n\n#fig = plt.subplots(figsize=(7, 4))\n#cmap = 'Reds'\n#cmap = 'YlGnBu'\n#cmap = 'YlOrBr'\n#cmap = 'BuGn_r'\n#cmap = 'RdYlBu'\n#cmap = 'GnBu'\n#cmap=\"OrRd\"\n#sns.heatmap(corr, cmap=cmap)\n\n#plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Forecast"},{"metadata":{},"cell_type":"markdown","source":"### Looking at solar energy and phasor angle, can you forecast phase angle"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df2 = df[['time', 'phasor_diff1', 'PV_hoch']].copy()\nnew_df2 = new_df2.set_index(['time'])\nnew_df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start and end dates of the time series\nprint (\"Dataset: start_date = {}, end_date = {} \\n\".format(new_df2.index.min(), new_df2.index.max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the supervised learning algorithm was adopted from\n#https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdff = pd.DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(dff.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(dff.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = pd.concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Scale all features in range of [0,1].\nvalues = new_df2.values \n\n#features normalization\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 18)\n#print(reframed.head())\n# columns we don't need to predict are dropped\n#reframed.drop(reframed.columns[[3,5,7,9]], axis=1, inplace=True)\nreframed.drop(reframed.columns[[3,5,7,9,11,13,15,17,19,21,23,25,27,29,30,31,33,35,37]], axis=1, inplace=True)\nprint(reframed.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test sets\nvalues = reframed.values\n\ntrain_size = int(len(values) * 0.67)\ntest_size = len(values) - train_size\ntrain, test = values[0:train_size,:], values[train_size:len(scaled),:]\n\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape) \n# The input is reshaped into a 3D format as expected by LSTMs, namely [samples, timesteps, features].","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(128, activation='relu',input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64, activation='relu',return_sequences=False))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mse')\n#model.summary()\n\n# fit model\nhistory = model.fit(train_X, train_y, epochs=20, batch_size=70, validation_data=(test_X, test_y), verbose=1, shuffle=False)\n\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Training loss', 'Validation loss'], loc='upper right')\n\nplt.show()\n\n# make a prediction\nvhat = model.predict(train_X) ## train\nyhat = model.predict(test_X) ## test\ntest_X = test_X.reshape((test_X.shape[0], 18))\ntrain_X = train_X.reshape((train_X.shape[0], 18))\n\n# invert scaling for forecast (train)\ninv_vhat = np.concatenate((vhat, train_X[:, -1:]), axis=1)\ninv_vhat = scaler.inverse_transform(inv_vhat)\ninv_vhat = inv_vhat[:,0]\n\n# invert scaling for forecast (test)\ninv_yhat = np.concatenate((yhat, test_X[:, -1:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual (train)\ntrain_y = train_y.reshape((len(train_y), 1))\ninv2_y = np.concatenate((train_y, train_X[:, -1:]), axis=1)\ninv2_y = scaler.inverse_transform(inv2_y)\ninv2_y = inv2_y[:,0]\n\n# invert scaling for actual (test)\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_X[:, -1:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n\n# calculate RMSE and MAE train and test\nrmse2 = np.sqrt(mean_squared_error(inv2_y, inv_vhat))\nprint('Train RMSE: %.3f' % rmse2)\nmae2 = mean_absolute_error(inv2_y, inv_vhat)\nprint('Train MAE: %.3f' % mae2)\n\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)\nmae = mean_absolute_error(inv_y, inv_yhat)\nprint('Test MAE: %.3f' % mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## time steps, every step is 15 min (you can easily convert the time step to the actual time index)\n## for a demonstration purpose, I only compare the predictions in 50 hours. \n\naa=[x for x in range(100)]\n\nplt.plot(aa,inv_y[:100], marker='.', label=\"Actual data\")\nplt.plot(aa,inv_yhat[:100], '--r', label=\"Predicted data\")\n# plt.tick_params(left=False, labelleft=True) #remove ticks\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('Phasor difference', size=15)\nplt.xlabel('Time step', size=15)\nplt.legend(fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}